{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WnSj5eR2xR3P",
        "outputId": "53ccb268-0720-4987-d450-125e58f9c00c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        }
      ],
      "source": [
        "# prompt: i have dataframe true and predicted texts, i need to apply blue, meteor and rouge scores\n",
        "%%capture\n",
        "!pip install nltk rouge-score sacrebleu\n",
        "\n",
        "import nltk\n",
        "from rouge_score import rouge_scorer\n",
        "from sacrebleu.metrics import BLEU\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "\n",
        "# Example usage:\n",
        "# Assuming you have a DataFrame called 'df' with 'true_text' and 'predicted_text' columns\n",
        "# df = calculate_scores(df)\n",
        "# print(df)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "def calculate_scores(df , true , pred):\n",
        "  \"\"\"Calculates BLEU, METEOR, and ROUGE scores for a dataframe.\n",
        "\n",
        "  Args:\n",
        "    df: A Pandas DataFrame containing 'true_text' and 'predicted_text' columns.\n",
        "\n",
        "  Returns:\n",
        "    A new DataFrame with BLEU, METEOR, and ROUGE scores appended.\n",
        "  \"\"\"\n",
        "\n",
        "  bleu_scores = []\n",
        "  meteor_scores = []\n",
        "  rouge_scores = []\n",
        "  bleu1_scores = []\n",
        "  bleu2_scores = []\n",
        "  bleu3_scores = []\n",
        "  bleu4_scores = []\n",
        "\n",
        "  scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
        "\n",
        "  for _, row in df.iterrows():\n",
        "    true_text = row[true]\n",
        "    predicted_text = row[pred]\n",
        "\n",
        "    # reference = [['this', 'is', 'very, 'small', 'test']]\n",
        "    # candidate = ['this', 'is', 'a', 'test']\n",
        "    # sentence_bleu(reference, candidate)\n",
        "    reference = [true_text.split()]\n",
        "    candidate = predicted_text.split()\n",
        "    bleu1 =  sentence_bleu(reference, candidate, weights=(1, 0, 0, 0))\n",
        "    bleu2 = sentence_bleu(reference, candidate, weights=(0.5, 0.5, 0, 0))\n",
        "    bleu3 = sentence_bleu(reference, candidate, weights=(0.33, 0.33, 0.33, 0))\n",
        "    bleu4 = sentence_bleu(reference, candidate, weights=(0.25, 0.25, 0.25, 0.25))\n",
        "    # bleu = sentence_bleu( [true_text.split()] , predicted_text.split() )\n",
        "    bleu1_scores.append(bleu1)\n",
        "    bleu2_scores.append(bleu2)\n",
        "    bleu3_scores.append(bleu3)\n",
        "    bleu4_scores.append(bleu4)\n",
        "\n",
        "    # METEOR Score\n",
        "    # print (nltk.translate.meteor_score.meteor_score(\n",
        "    # [\"this is an apple\", \"that is an apple\"], \"an apple on this tree\"))\n",
        "    # try:\n",
        "    meteor_score = nltk.translate.meteor_score.single_meteor_score(true_text.split() , predicted_text.split() )\n",
        "    meteor_scores.append(meteor_score)\n",
        "    # except:\n",
        "    #   print(32)\n",
        "    #   meteor_scores.append(0)  # Handle potential errors\n",
        "\n",
        "\n",
        "    # ROUGE Score\n",
        "    rouge = scorer.score(true_text, predicted_text)\n",
        "    rouge_scores.append(rouge)\n",
        "\n",
        "  all_scored = {}\n",
        "  all_scored['bleu1'] = bleu1_scores\n",
        "  all_scored['bleu2'] = bleu2_scores\n",
        "  all_scored['bleu3'] = bleu3_scores\n",
        "  all_scored['bleu4'] = bleu4_scores\n",
        "  all_scored['meteor'] = meteor_scores\n",
        "  all_scored['rouge1'] = [score['rouge1'].fmeasure for score in rouge_scores]\n",
        "  all_scored['rouge2'] = [score['rouge2'].fmeasure for score in rouge_scores]\n",
        "  all_scored['rougeL'] = [score['rougeL'].fmeasure for score in rouge_scores]\n",
        "\n",
        "  return all_scored"
      ],
      "metadata": {
        "id": "ua4ACbm1zEpc"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv('/content/llama_dpo_sft_samps300_urdu2eng.csv')\n",
        "df.columns"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R1MClHg9zOUu",
        "outputId": "183f702c-a0f8-4569-e0a2-a9f79d10729e"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['trans', 'orginal_eng', 'Urdu', 'text', 'token_count',\n",
              "       'untrianed_translation', 'trianed_translation', 'prompt', 'chosen',\n",
              "       'rejected', 'dpo+sft_trans', 'dpo_sft_trans'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "all_scores = calculate_scores(df.tail(25) , 'orginal_eng' , 'trianed_translation')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-DBsniEOzewG",
        "outputId": "421369ff-ed7f-441a-bf0b-07f3baf6328a"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
            "The hypothesis contains 0 counts of 3-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.10/dist-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
            "The hypothesis contains 0 counts of 4-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: get last 25 rows of df, write a simple single line code\n",
        "\n",
        "df.tail(25).shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i2JLI7L4_Hi_",
        "outputId": "88814e52-b093-4823-9be1-4ba2643a2514"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(25, 12)"
            ]
          },
          "metadata": {},
          "execution_count": 85
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(all_scores['bleu2'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m0uuY6is2P5k",
        "outputId": "f9ed80ce-ffcd-4a4e-b6a5-f1e4d11973b0"
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "25"
            ]
          },
          "metadata": {},
          "execution_count": 87
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "scores ={}\n",
        "for key, value in all_scores.items():\n",
        "    scores[key] = np.array(value).mean()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "JqBUfgbGzt6k"
      },
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scores"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wV0MLmOu0hu4",
        "outputId": "44fe09bf-e07a-41eb-bd51-667e4d48e3ac"
      },
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'bleu1': 0.2537656352702947,\n",
              " 'bleu2': 0.0904179191702786,\n",
              " 'bleu3': 0.0180240658981879,\n",
              " 'bleu4': 0.003128094415461956,\n",
              " 'meteor': 0.18298543641873904,\n",
              " 'rouge1': 0.30372905380613135,\n",
              " 'rouge2': 0.04523623316803908,\n",
              " 'rougeL': 0.2005644198596401}"
            ]
          },
          "metadata": {},
          "execution_count": 89
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "all_scores_dpo = calculate_scores(df, 'orginal_eng' , 'dpo_sft_trans')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2MMiEMfD77oX",
        "outputId": "0cf3a408-3594-4bd5-8f22-746d232d1990"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
            "The hypothesis contains 0 counts of 3-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.10/dist-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
            "The hypothesis contains 0 counts of 4-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "scores_dpo ={}\n",
        "for key, value in all_scores_dpo.items():\n",
        "    scores_dpo[key] = np.array(value).mean()\n",
        "scores_dpo , scores"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YTlma47b8Rz3",
        "outputId": "737430e2-bc11-4aa5-b938-2e7f797f6bb6"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "({'bleu1': 0.25597558397333864,\n",
              "  'bleu2': 0.1026252416733309,\n",
              "  'bleu3': 0.03022041424994214,\n",
              "  'bleu4': 0.01046593389246158,\n",
              "  'meteor': 0.20991123979160528,\n",
              "  'rouge1': 0.33529334500393526,\n",
              "  'rouge2': 0.05451672941720152,\n",
              "  'rougeL': 0.22437770225528167},\n",
              " {'bleu1': 0.2609844261461966,\n",
              "  'bleu2': 0.10383926012482969,\n",
              "  'bleu3': 0.028501688505826524,\n",
              "  'bleu4': 0.010048530912659852,\n",
              "  'meteor': 0.21083288725293767,\n",
              "  'rouge1': 0.33664732938454867,\n",
              "  'rouge2': 0.05701932631641834,\n",
              "  'rougeL': 0.2225822267201796})"
            ]
          },
          "metadata": {},
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "all_scores_gpt = calculate_scores(df.tail(25), 'orginal_eng' , 'trans')\n",
        "scores_gpt ={}\n",
        "for key, value in all_scores_gpt.items():\n",
        "    scores_gpt[key] = np.array(value).mean()\n",
        "\n",
        "scores_gpt , scores"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UJu0Igs-8XzW",
        "outputId": "06eb1bea-5783-420d-c1f9-94e66b3cf03d"
      },
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
            "The hypothesis contains 0 counts of 3-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.10/dist-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
            "The hypothesis contains 0 counts of 4-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.10/dist-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
            "The hypothesis contains 0 counts of 2-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "({'bleu1': 0.2523791496767713,\n",
              "  'bleu2': 0.09666527898920892,\n",
              "  'bleu3': 0.032711989350160134,\n",
              "  'bleu4': 0.006811041608997601,\n",
              "  'meteor': 0.19673154688792718,\n",
              "  'rouge1': 0.4068814969964297,\n",
              "  'rouge2': 0.08161020613354937,\n",
              "  'rougeL': 0.26654801825569724},\n",
              " {'bleu1': 0.2537656352702947,\n",
              "  'bleu2': 0.0904179191702786,\n",
              "  'bleu3': 0.0180240658981879,\n",
              "  'bleu4': 0.003128094415461956,\n",
              "  'meteor': 0.18298543641873904,\n",
              "  'rouge1': 0.30372905380613135,\n",
              "  'rouge2': 0.04523623316803908,\n",
              "  'rougeL': 0.2005644198596401})"
            ]
          },
          "metadata": {},
          "execution_count": 90
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "all_scores_ut = calculate_scores(df, 'orginal_eng' , 'untrianed_translation')\n",
        "scores_ut ={}\n",
        "for key, value in all_scores_ut.items():\n",
        "    scores_ut[key] = np.array(value).mean()\n",
        "\n",
        "scores_ut"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LfHCUbGM9Wl0",
        "outputId": "3798c3fa-aa38-4b39-faf4-e0bd786c02f5"
      },
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
            "The hypothesis contains 0 counts of 2-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.10/dist-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
            "The hypothesis contains 0 counts of 3-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.10/dist-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
            "The hypothesis contains 0 counts of 4-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'bleu1': 0.1799328013890531,\n",
              " 'bleu2': 0.042656479100743175,\n",
              " 'bleu3': 0.006166470870192632,\n",
              " 'bleu4': 0.001272398789375698,\n",
              " 'meteor': 0.1439213395000858,\n",
              " 'rouge1': 0.2951635731077196,\n",
              " 'rouge2': 0.03502346122931948,\n",
              " 'rougeL': 0.1822686491789083}"
            ]
          },
          "metadata": {},
          "execution_count": 91
        }
      ]
    }
  ]
}
